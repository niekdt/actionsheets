date = 2024-09-16
partial = true
language = "python"
parent = "python.pandas"
name = "dataframe"
title = "Pandas DataFrame"
description = """
Opinionated words of caution:

* Data frames force a frustrating distinction between the "data" and index columns, despite this being completely meaningless in typical exploratory data analysis applications where what constitutes an index is highly context-dependent. Also, pandas is very slow regardless, so all effort on making effective use of index columns is a waste of time.
* Don't bother with index columns unless you want to polute your code with many `reset_index()` calls, or risk surprising results.
* On the other hand, you cannot ignore index columns, as you'll run into vague index errors whether you want it or not (e.g., assigning groupby results, shuffling rows).
* MultiIndex versus multiple indices. Just don't. If you cared about speed you would not be using pandas anyway.
* Many conflicting dtypes, inconsistencies in API functions for handling them, and ad-hoc mixing with numpy functions.
* Complex queries will be a series of data variable updates, which is hard to read, and guaranteed to lead to bugs at a later stage during refactoring.
* Inconsistent APIs between data frames, columns, groupby output, and index columns.
* Because of all the complexities above, writing good and clean pandas code is very hard, as is evident by the large volume of unreadable/anti-pattern-riddled pandas code online (e.g., stackoverflow).
"""
code = "import pandas as pd"


[create]
section = "Create"

[create.empty]
action = "Empty"
code = "?"

[create.list]
action = "Single column from list"
code = "pd.DataFrame([1, 2, 3])"

[create.lists]
action = "From column lists"
code = "pd.DataFrame({'A': [1, 2], 'fruits': ['banana', 'apple']})"

[create.dict]
action = "From dictionary with column list entries"
code = """
dict = {'Name': ['John', 'Sue'], 'Age': [40, 35]}
pd.DataFrame(dict)
"""

[create.numpy]
action = "From Numpy 2D array"
code = """
import numpy as np
arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
pd.DataFrame(arr, columns = ['a', 'b', 'c'])
"""

[create.str.json]
action = "From JSON string"
code = """
import json
json_dict = json.loads(json_string)
pd.DataFrame(json_dict)
"""

[create.file.json]
action = "From JSON file"
code = "pd.read_json(path)"


[test]
section = "Test"

[test.inherits]
action = "Is data frame or subclass"
code = "isinstance(data, pd.DataFrame)"

[test.is]
action = "Is data frame and not subclass"
code = "type(data) is pd.DataFrame"

[test.empty]
action = "Is empty"
code = "data.empty"

[test.exists]
section = "Test for presence of columns"

[test.exists.col]
action = "Has column _col_"
code = "col in data"

[test.exists.col.all]
action = "Has columns _cols_"
code = "data.columns.isin(cols).all()"

[test.exists.col.all.set]
code = "set(cols).issubset(data)"

[test.exists.cols.only]
action = "Only contains columns _cols_"
code = "set(data) == set(cols)"

[test.exists.cols.any]
action = "Has any of these columns _cols_"
code = "data.columns.isin(cols).any()"

[test.exists.cols.any.set]
code = "set(data).issuperset(cols)"

[test.exists.cols.none]
action = "Does not have these columns _cols_"
code = "set(cols).isdisjoint(data)"


[test.type]
section = "Tests for column types"

[test.type.bool]
action = "Is column _col_ boolean type"
code = "pd.api.types.is_bool_dtype(data[col])"

[test.type.categorical]
action = "Is column _col_ categorical type"
code = "isinstance(data[col], pd.CategoricalDtype)"

[test.type.str]
action = "Is column _col_ string type"
code = "pd.api.types.is_string_dtype(data[col])"

[test.type.numeric]
action = "Is column _col_ numeric type"
code = "pd.api.types.is_numeric_dtype(data[col])"

[test.type.int]
action = "Is column _col_ integer type"
code = "pd.api.types.is_integer_dtype(data[col])"

[test.type.datetime]
action = "Is column _col_ datetime type"
code = "pd.api.types.is_datetime64_dtype(data[col])"

[test.type.datetime.ns]
action = "Is column _col_ datetime(nanosecond) type"
code = "data.dtypes[col] == numpy.dtype('datetime64[ns]'))"

[test.value.col.na.any]
action = "Does column _col_ contain NA value(s)"
code = "data[col].isna().values.any()"


[test.contains]
section = "Tests for values"

[test.contains.na]
action = "Contains NA"
code = "data.isna().any().any()"

[test.contains.na.none]
action = "Contains no NA"
code = "data.notna().all().all()"

[test.contains.na.col]
action = "Does column _col_ contain any NA values"
code = "data[col].hasnan"

[test.contains.na.col.none]
action = "Does column _col_ not contain NA values"
code = "~data[col].hasnan"

[test.contains.na.cols.none]
action = "Do none of the columns _cols_ contain NA values"
code = "data[['col1', 'col2']].notnull().all().all()"

[test.contains.'inf']
action = "Contains any infinity"
code = """
import numpy as np
data.isin([np.inf, -np.inf]).values.any()
"""

[test.contains.'inf'.col]
action = "Does column _col_ contain any infinite values"
code = """
import numpy as np
data['col'].isin([np.inf, -np.inf]).any()
"""

[test.contains.'inf'.col.2]
code = """
import numpy as np
np.isinf(data['col']).any()
"""


[test.duplicate]
section = "Tests for duplicates"

[test.duplicate.row.none]
action = "No duplicated rows (all unique)"
code = "~data.duplicated().any()"

[test.duplicate.row.any]
action = "Some rows are duplicated"
code = "data.duplicated().any()"

[test.duplicate.row.grouped.none]
action = "No groups in column _groupCol_ have duplicated values for column _valueCol_"
code = "~data.duplicated(subset=['groupCol', 'valueCol']).any()"

[test.duplicate.row.grouped]
action = "Some groups in column _groupCol_ have duplicate elements in column _valueCol_"
code = "data.duplicated(subset=['groupCol', 'valueCol']).any()"

[test.duplicate.cols.none]
action = "No columns are duplicated by values"
code = "?"

[test.duplicate.cols.any]
action = "Some columns are duplicate by values"
code = "?"


[test.assert]
section = "Assertions"

[test.assert.frame.equal]
action = "Assert frames are equal"
code = "pd.testing.assert_frame_equal(x, y)"

[test.assert.frame.equal.order]
action = "Assert frames are equal, ignoring column order and row order"
code = "pd.testing.assert_frame_equal(x, y, check_like=True)"


[extract]
section = "Extract"
description = "Snippets which return non-DataFrame output (e.g., scalar, series)"

[extract.row.count]
action = "Number of rows"
code = "len(data)"

[extract.row.count.shape]
code = "data.shape[0]"

[extract.col.names]
action = "Column names"
code = "list(data)"

[extract.row.names]
action = "Row names"
code = "data.index"

[extract.col.count]
action = "Number of columns"
code = "len(data.columns)"

[extract.col.count.shape]
code = "data.shape[1]"

[extract.cell.count]
action = "Number of cells"
code = "data.size"

[extract.dim]
action = "Dimensions (as tuple)"
code = "data.shape"

[extract.col]
action = "Get column named _name_ (as series)"
code = "data['name']"

[extract.rowmask]
section = "Extract row masks"

[extract.rowmask.na]
action = "Mask for missing values (None, NA, NaN)"
code = "data.isna()"

[extract.rowmask.notna]
action = "Mask for non-missing values"
code = "data.notna()"
details = "Same result for `data.notnull()`?"

[extract.rowmask.duplicate]
action = "Mask for duplicate rows"
code = "data.duplicated()"

[extract.rowmask.duplicate.cols]
action = "Mask for duplicates across columns in list _cols_"
code = "data.duplicated(subset=cols)"

[extract.row.count.group]
action = "Extract row count by group _group_"
code = "data.groupby('group').size().reset_index(name='count')"
details = "Returns a data frame with group columns and a respective count column"


[query]
section = "Query"

[query.cell.at]
action = "Cell at row _i_, col _j_"
code = "data.at[i, j]"

[query.row.first]
action = "First row (Series)"
code = "data.iloc[0]"

[query.row.at]
action = "Row by row number _i_"
code = "data.iloc[i]"

[query.row.index]
action = "Row by index _i_"
code = "data.loc[i]"

[query.str]
action = "String query"
code = "data.query(...)"


[update]
section = "Update"
description = """
All operations are in-place.
Transformed columns can be assigned to new frames, but this operation is generally unsafe as it depends on the index columns matching.
"""


[update.transform]
section = "Transform"

[update.transform.col]
section = "Transform a single column"

[update.transform.col.rename]
action = "Rename column"
code = "data.rename(columns={'old': 'new'}, inplace=True)"

[update.transform.col.datetime]
action = "Set datetime column _date_ to standard unit (ns)"
code = "data['date'] = data['date'].apply(pd.to_datetime)"
details = "Needed, for example, to avoid data corruption when saving to HDF5 store"

[update.transform.col.sample.group]
action = "Sample a column _col_ per grouping in _group_ (ignore index)"
code = """
data[col] = data.groupby(group)[col].transform(
    lambda x: x.sample(frac=1).to_list()
)
"""
details = "`to_list()` is essential here to force pandas to ignore the index columns, otherwise there is no effect from sampling..."

[update.transform.col.list.apply]
action = "Apply function to a list column _myCol_ (as flattened series)"
code = """
flat_data = data.explode('myCol')
flat_data.index.name = '_index'
data['myCol'] = flat_data['myCol'].groupby('_index').agg(lambda x: x)
"""
details = """
Warning: empty lists are converted to NaN (??)
Setting the index name is required because pandas cannot group by nameless index...
"""


[update.transform.cols]
section = "Transform multiple columns"

[update.transform.cols.rename]
action = "Rename columns"
code = "data.rename(columns={'old1': 'new1', 'old2': 'new2'}, inplace=True)"

[update.transform.cols.rename.list]
action = "Rename columns based on a list of old names and list of new names"
code = "data.rename(columns=dict(zip(oldNames, newNames)), inplace=True)"

[update.transform.cols.rename.lower]
action = "All columns to lowercase"
code = "data.rename(str.lower, axis='columns', inplace=True)"


[update.grow]
section = "Grow"

[update.grow.col.append]
action = "Append series as column _col_"
code = "data['col'] = s"
details = "Overwrites existing column"

[update.grow.col.append.cat.na]
action = "Append undefined categorical column _col_"
code = "data['col'] = pd.Categorical([None] * len(data), categories=['a', 'b', 'c'])"

[update.grow.col.append.str.concat]
action = "Add string column _col_ based on concatenation of two string columns _x_ and _y_, with separator _sep_"
code = "data['col'] = data['x'] + 'sep' + data['y']"

[update.grow.rownum]
action = "Add column with row number (ignore index)"
code = "data['num'] = range(len(data))"
details = 'Starts from 1'

[update.grow.rownum.group]
action = "Add column with row number per grouping in _group_ (ignore index)"
code = "data['num'] = data.groupby('group').cumcount().add(1)"
details = 'Starts from 1'

[update.cell.at]
action = "Set cell at row _i_, col _j_ to value _v_"
code = "data.at[i, j] = v"

[update.cell.missing.fill]
action = "Replace missing values for _v_"
code = "data.fillna(v)"

[update.cell.missing.cond]
action = "Replace specific values"
code = "data.replace(?)"


[derive]
section = "Derive"
description = "All operations create a new DataFrame."


[derive.transform]
section = "Transform"


[derive.transform.col]
section = "Transform single column"

[derive.transform.col.rename]
action = "Rename column"
code = "data.rename(columns={'old': 'new'})"

[derive.transform.col.rename.multi]
action = "Rename multiple columns"
code = "data.rename(columns={'old1': 'new1', 'old2': 'new2'})"

[derive.transform.col.rename.dyn]
action = "Rename columns dynamically"
code = "data.rename(columns=dict(zip(oldNames, newNames)))"

[derive.transform.col.rename.lower.all]
action = "All columns to lowercase"
code = "data.rename(str.lower, axis='columns')"


[derive.grow]
section = "Grow"

[derive.grow.col.append]
action = "Append column _col_ from series _s_"
code = "df.assign(col=s)"

[derive.grow.cols.append]
action = "Append columns"
code = "data.assign(s1, s2)"

[derive.grow.cols.append.concat]
code = "pd.concat(data, [s1, s2], axis=1)"

[derive.grow.col.insert]
action = "Insert column"
code = "data.insert(x)"

[derive.grow.col.str.split]
action = "Split string column _col_ into two columns based on separator _sep_"
code = """
data['col'].str.split('sep', n=1, expand=True)
"""


[derive.shrink]
section = "Shrink"

[derive.shrink.row]
section = "Reduce number of rows"

[derive.shrink.row.head]
action = "Keep first _n_ rows"
code = "data.head(n)"

[derive.shrink.row.remove.head]
action = "Remove first _n_ rows"
code = "data[n:]"

[derive.shrink.row.tail]
action = "Keep last _n_ rows"
code = "data.tail(n)"

[derive.shrink.row.remove.tail]
action = "Remove last _n_ rows"
code = "data[:-n]"

[derive.shrink.row.pop]
action = "Pop row"
code = "data.pop()"

[derive.shrink.row.largest]
action = "Select _n_ largest rows according to column _col_"
code = "data.nlargest(n, col)"

[derive.shrink.row.largest.group]
action = "Select _n_ largest rows per group _group_, according to column _col_"
code = "data.groupby('group').apply(lambda x: x.nlargest(n=n, columns='col')).reset_index(drop=True))"

[derive.shrink.row.last.group]
action = "Select last row per group _group_"
code = "data.groupby('group').last().reset_index()"

[derive.shrink.row.max.group]
action = "Select the row per group _group_ with the largest value for column _col_"
code = """
data.loc[data.groupby('group')['col'].idxmax()].reset_index(drop=True)
"""

[derive.shrink.row.remove.list]
action = "Remove rows list _rows_"
code = "data.drop(rows)"

[derive.shrink.row.remove.duplicate]
action = "Remove duplicated rows"
code = "data.drop_duplicates()"

[derive.shrink.row.remove.missing.cols]
action = "Remove rows with a missing value in any column"
code = "data.dropna()"

[derive.shrink.row.remove.missing.cols.subset]
action = "Remove rows with a missing value in the given columns list _cols_"
code = "data.dropna(subset=cols)"

[derive.shrink.row.remove.missing.cols.n]
action = "Remove rows with at least _n_ non-missing values across columns"
code = "data.dropna(thresh=n)"

[derive.shrink.row.remove.missing.col.all]
action = "Remove rows with missing values in every column"
code = "data.dropna(how='all')"


[derive.shrink.col]
section = "Reduce number of columns"

[derive.shrink.col.keep]
action = "Select single column named _subject_"
code = "data[['subject']]"
details = "Note the double brackets"

[derive.shrink.cols.keep]
action = "Select columns"
code = "data[['subject', 'date']]"

[derive.shrink.col.remove.duplicate]
action = "Remove duplicated columns"
code = "?"

[derive.shrink.col.remove]
action = "Remove column _col_"
code = "data.drop(columns=col)"

[derive.shrink.col.remove.try]
action = "Remove column _col_ if it exists"
code = "data.drop(columns=col, errors='ignore')"

[derive.shrink.col.remove.multi]
action = "Remove columns list _cols_"
code = "data.drop(columns=cols)"

[derive.shrink.col.missing.row.any]
action = "Remove columns with a missing value in any row"
code = "data.dropna(axis='columns')"


[derive.reshape]
section = "Reshape"

[derive.reshape.melt]
action = "Melt"
code = "data.melt(?)"

[derive.reshape.dcast]
action = "Dcast"
code = "data.explode(?)"

[derive.reshape.transpose]
action = "Transpose"
code = "data.T"


[derive.combine]
section = "Combine"

[derive.combine.concat]
action = "Concatenate rows of dataframes"
code = "pd.concat([df, df2, dfN])"
details = "Consider `ignore_index=True` argument"

[derive.combine.concat.mix]
action = "Concatenate rows of dataframes, having partially overlapping columns"
code = "?"

[derive.combine.concat.cols]
action = "Concatenate columns of dataframes, assuming equal index, and assuming columns don't overlap"
code = "pd.concat([data, data2], axis=1)"
details = "Unsafe. First make sure indexes are aligned, or output has twice the number of rows"

[derive.combine.concat.cols.safe]
action = "Concatenate columns of dataframes, ignoring the index, and assuming columns don't overlap"
code = "pd.concat([data.reset_index(drop=True), data2.reset_index(drop=True)], axis=1)"

[derive.combine.merge.inner]
action = "Inner join"
code = "data.merge(data2, on=['sex', 'country'])"

[derive.combine.merge.left]
action = "Left join"
code = "data.merge(data2, on=['sex', 'country'], how='left')"

[derive.combine.merge.right]
action = "Right join"
code = "data.merge(data2, on=['sex', 'country'], how='right')"

[derive.combine.merge.outer]
action = "Outer (full) join"
code = "data.merge(data2, on=['sex', 'country'], how='outer')"

[derive.combine.merge.cross]
action = "Cross join"
code = "data.merge(data2, on=['sex', 'country'], how='cross')"

[derive.combine.merge.anti.left]
action = "Left anti join"
code = """
outer_data = data.merge(data2, on=['sex', 'country'], how='outer', indicator=True)
outer_data[outer_data._merge == 'left_only'].drop('_merge', axis=1)
"""

[derive.combine.merge.anti.right]
action = "Left anti join"
code = """
outer_data = data.merge(data2, how='outer', indicator=True)
outer_data[outer_data._merge == 'right_only'].drop('_merge', axis=1)
"""


[iter]
section = "Iterate"

[iter.row.tuple]
action = "Over rows (as tuples)"
code = "for row in data.itertuples():"

[iter.row.enum]
action = "Over rows (index, series)"
code = "for i, row in data.iterrows():"

[iter.col]
action = "Over columns (lists)"
code = "for col in data.columns:"


[convert]
section = "Convert"

[convert.dict.list]
action = "To dict of column lists"
code = "data.to_dict('list')"

[convert.dict.series]
action = "To dict of column series"
code = "recodata.to_dict('series')"

[convert.list.dict]
action = "To list of dict per row"
code = "data.to_dict('records')"

[convert.numpy]
action = "To Numpy 2D array"
code = """
import numpy as np
data.to_numpy()
"""

[convert.polars]
action = "To polars DataFrame"
code = """
import polars as pl
import pyarrow
pl.from_pandas(data)
"""

[convert.str.rows]
action = "To pretty string, with at most _n_ rows"
code = "data.to_string(index=False, max_rows=n)"


[convert.file]
section = "To file format"

[convert.file.csv]
action = "To CSV file"
code = "data.to_csv('file.csv', index=False)"
details = "`index=False` is needed not to polute CSV with a meaningless index"

[convert.file.tsv]
action = "To TSV file"
code = "data.to_csv('file.tsv', sep='\t', index=False)"
details = "`index=False` is needed not to polute TSV with a meaningless index"

[convert.file.json]
action = "To JSON file"
code = "data.to_json('file.json', orient)"
details = "See docs to determine orientation"
source = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html"

[convert.file.parquet]
action = "To parquet"
code = "data.to_parquet('file.parquet')"


[config]
section = "Options"

[config.option]
action = "Set an option _opt_ to _value_"
code = "pd.set_option('opt', value)"
source = "https://pandas.pydata.org/docs/reference/api/pandas.set_option.html"

[config.option.temp]
action = "Use options within a context"
code = """
from pandas import option_context
with option_context('display.max_rows', 10, 'display.max_columns', None):
    print(data)
"""
source = "https://pandas.pydata.org/docs/reference/api/pandas.option_context.html"

[config.display.columns]
action = "Show all columns"
code = "pd.options.display.max_columns = None"

[config.display.rows]
action = "Show all rows"
code = "pd.options.display.max_rows = None"

[config.display.width]
action = "Set max output width, in characters"
code = "pd.options.display.width = 120"
details = "Default is 80"

[config.display.float]
action = "Show floats with _d_ decimal digits precision"
code = "pd.options.display.precision = d"

[config.display.comma]
action = "Format numbers with thousand separator"
code = "pd.options.styler.format.thousands = ','"
