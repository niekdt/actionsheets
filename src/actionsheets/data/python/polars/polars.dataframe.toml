language = "python"
parent = "python.polars"
name = "dataframe"
title = "Polars DataFrame"
description = "The polars package provides a fast and powerful implementation of data frames"
details = "Some more details here"
code = "import polars as pl"

[create]
section = "Create"
description = "Create a new DataFrame"

[create.dataframes.concat]
action = "From dataframes (concatenate)"
code = "pl.concat([df, df2, dfN])"

[create.dataframes.concat.mix]
action = "From dataframes with different columns"
code = "pl.concat([df, df2, dfN], how = 'diagonal')"

[create.dataframe.arange]
action = "Repeat each row with an index between a row-specific range, defined by min (inclusive) and max (exclusive) columns"
code = """
data.select(
    pl.col('patient'),
    pl.int_ranges(start='startDay', end='endDay').alias('day')
).explode('day')
"""

[create.lists]
action = "From column lists"
code = "pl.DataFrame({'A': [1, 2], 'fruits': ['banana', 'apple']})"

[create.list.lists]
action = "From list of lists"
code = """
data = [[1, 2, 3], [4, 5, 6]]
pl.DataFrame(data, schema=['a', 'b', 'c'])
"""

[create.dict]
action = "From dict"
code = "pl.DataFrame(dict)"

[create.dict.schema]
action = "From dict with schema"
code = """
pl.DataFrame(
    dict,
    schema = {'col1': pl.Float32, 'col2': pl.Int64, 'col3': pl.Date}
)
"""

[create.pandas]
action = "From `pandas.DataFrame`"
code = "pl.from_pandas(data)"

[create.nparray]
action = "From numpy array"
code = """
data = np.array([[1, 2], [3, 4]])\n
pl.DataFrame(data, schema = ['a', 'b'], orient = 'col')
"""

[create.file]
section = "From file formats"

[create.file.csv]
action = "From CSV file"
code = "pl.read_csv('file.csv')"

[prop]
section = "Properties"

[prop.ncol]
action = "Number of columns"
code = "len(data.columns)"

[prop.col.names]
action = "Column names"
code = "data.columns"

[prop.col.types]
action = "Column types"
code = "data.dtypes"

[prop.col.types.map]
action = "Column-types mapping"
code = "data.schema"

[prop.col.which]
action = "Find column index by name"
code = "data.find_idx_by_name('age')"

[prop.nrow]
action = "Number of rows"
code = "data.height"


[test]
section = "Test"

[test.empty]
action = "Empty (no rows)"
code = "data.is_empty()"

[test.equal]
action = "Data frames are equal"
code = "data.equals(data2)"


[test.col]
section = "Columns"

[test.col.contains]
action = "Contains column"
code = "'age' in data.columns"

[test.col.contains.multi]
action = "Contains columns"
code = "{'age', 'sex'}.issubset(data.columns)"

[test.col.contains.dyn]
action = "Contains columns _cols_"
code = "set(cols).issubset(data.columns)"

[test.col.missing]
action = "Column is missing"
code = "'age' not in data.columns"

[test.col.equals]
action = "Columns are equal"
code = "?"

[test.col.equal.series]
action = "Columns are equal series"
code = "data['sex'].equals(data['sex2'].alias('sex'))"
details = "Series names must match!"

[test.col.missing.any]
action = "Column has missing value"
code = "data['sex'].is_null().any()"

[test.col.missing.none]
action = "Column has no missing values"
code = "data['sex'].is_not_null().all()"

[test.col.duplicate.none]
action = "Column has no duplicate values"
code = "?"

[test.col.duplicate.any]
action = "Column has duplicate values"
code = "?"

[test.col.type]
action = "Column is of type"
code = "data.schema['col1'] == dtype"

[test.col.type.bool]
action = "Column is bool type"
code = "data.schema['alive'] == pl.Bool"

[test.col.type.str]
action = "Column is string type"
code = "data.schema['sex'] == pl.Utf8"

[test.col.type.number]
action = "Columns is numeric"
code = "data.schema['age'].is_numeric()"

[test.col.type.int]
action = "Column is integer type"
code = "data.schema['age'].is_integer()"

[test.col.type.int64]
action = "Column is standard integer (64-bit)"
code = "data.schema['age'] == pl.Int64"

[test.col.type.float]
action = "Columns is float"
code = "data.schema['age'].is_float()"

[test.col.row.gt.all]
action = "Column is consistently rowwise greater than another column"
code = "(data['col1'] > data['col2']).all()"

[test.col.row.gt.any]
action = "Column is sometimes rowwise greater than another column"
code = "(data['col1'] > data['col2']).any()"


[test.rowmask]
section = "Row masking"

[test.rowmask.duplicate]
action = "Duplicated"
code = "data.is_duplicated()"

[test.rowmask.unique]
action = "Unique"
code = "data.is_unique()"


[query]
section = "Query"
description = "Start a lazy query using a LazyFrame by `data.lazy()`. Operations on a LazyFrame are not executed until this is requested by either calling `collect()` or `fetch()`. Lazy operations are advised because they allow for query optimization and more parallelization."

[query.col]
section = "Columns"
description = "Query the row(s) of one or more columns"

[query.col.single]
action = "Single column"
code = "data['col1']"
details = "Short, and works for literals and variables"

[query.col.single.attr]
code = "data.col1"
details = "Shortest"

[query.col.single.select]
code = "data.select('col1')"
details = "Verbose"

[query.col.multi]
action = "Multiple columns"
code = "data.select('col1', 'col2')"

[query.col.multi.var]
action = "Multiple columns, dynamically"
code = "data.select(['col1', 'col2'])"

[query.row]
section = "Rows"

[query.row.empty]
action = "Empty row"
code = "data.clear()"
details = "?"

[query.row.index]
action = "_i_ th row"
code = "data[i]"

[query.row.index.end]
action = "_i_ th row from end"
code = "data[-i]"

[query.row.head]
action = "First _n_ rows (head)"
code = "data.head(n)"

[query.row.tail]
action = "Last _n_ rows (tail)"
code = "data.tail(n)"

[query.row.slice]
action = "Slice of rows from _a_ to _b_"
code = "data[a:b]"

[query.row.slice.fun]
code = "data.slice(a, b)"

[query.row.index.list]
action = "By list of row numbers"
code = "data[rows]"

[query.row.exclude.index]
action = "Exclude the given row numbers"
code = "data.with_row_count().filter(pl.col('row_nr').is_in(rows).not_())"
details = "Leftover row_nr column"

[query.row.exclude.null]
action = "Exclude rows that contain null values"
code = "data.drop_nulls()"

[query.row.exclude.null.col]
action = "Exclude rows that contain null values in certain columns"
code = "data.drop_nulls(['fruits', 'cars'])"

[query.row.cond.col]
action = "Conditionally on column"
code = "data.filter(pl.col('age') >= 18)"

[query.row.cond.cols]
action = "From multiple column conditions"
code = "data.filter((pl.col('age') >= 18) & (pl.col('sex') == 'male'))"

[query.row.limit]
action = "Limit query to first _n_ rows"
code = "data.limit(n)"

[query.row.limit.tail]
action = "Limit query to last _n_ rows"
code = "data.limit(-n)"

[query.row.count.missing]
action = "Number of missing values"
code = "data.null_count()"

[query.row.count.unique.col]
action = "Number of unique values in a column"
code = "data['col1'].n_unique()"

[query.row.count.unique.col.multi]
action = "Number of unique rows across columns"
code = "?"

[query.aggregate]
section = "Aggregate"

[query.aggregate.group]
section = "By group"

[query.aggregate.group.mean]
action = "Mean of column by group"
code = "data.group_by('sex').agg(pl.col('age').mean())"

[query.aggregate.time]
section = "Over time"
description = "Aggregation is done at a fixed datetime interval (e.g., daily, hourly), based on an datetime index."

[query.aggregate.time.row]
section = "Row-based"
description = "Rolling computation for each row (i.e., observation in time)"

[query.aggregate.time.row.solo.right]
action = "Rolling computation for a single column, with right-aligned partial window of max _n_ days and at least _i_ rows"
code = """
data.with_columns(
    pl.col('Hospitalized').rolling_mean(
        by='Date',
        closed='both',
        window_size=f'{n - 1}d',
        min_periods=i
    )
)
"""
details = "Currently only available for `rolling_mean()`, not any of the other `rolling_` functions."

[query.aggregate.time.row.right]
action = "Rolling computation with right-aligned partial window of max _n_ days"
code = """
new_data = data.rolling(
    index_column='Date',
    period=f'{n}d'
).agg(
    pl.mean('Obs)
)
"""

[query.aggregate.time.row.right.keep]
action = "Rolling computation with right-aligned partial windows of max _n_ days, keep other columns"
code = """
new_data = data.rolling(
    index_column='Date',
    period=f'{n}d'
).agg(
    pl.exclude(['Date', 'Obs']).last(),
    pl.mean('Obs)
)
"""

[query.aggregate.time.interval]
section = "Interval-based"
description = "Rolling computation for constant (not dynamic!) intervals. This may introduce additional moments in time."

[query.aggregate.time.interval.right]
action = "Daily rolling computation with right-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='right',
    closed='both',
    start_by='window'
).agg(
    pl.mean('Obs')
)
"""
details = "To be verified"

[query.aggregate.time.interval.left]
action = "Daily rolling computation with left-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='left',
    closed='both',
    start_by='datapoint'
).agg(
    pl.mean('Obs')
)
"""
details = "To be verified"

[query.aggregate.time.interval.right.prop]
action = "Dynamic daily rolling statistic proportional to number of window observations, with right-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='right',
    closed='both',
    start_by='window'
).agg(
    pl.sum('Hospitalized').alias('DaysHospitalized'),
    pl.count()
).with_columns(
    Proportion=pl.col('DaysHospitalized') / pl.col('count')
)
"""
details = "To be verified"


[update]
section = "Update"
description = "Update the data frame in-place"

[update.col.replace]
action = "Replace column with another series"
code = "data.replace('age', newAgeSeries)"

[update.append.col.series]
action = "Append column from series"
code = "data.hstack(s, in_place = True)"

[update.insert.col.series]
action = "Insert column from series"
code = "data.insert_at_idx(1, s)"

[update.row.dataframe]
action = "Add a data frame"
code = "data.extend(data2)"

[update.row.dataframes]
action = "Add data frames"
code = """
data.vstack(data2)
data.vstack(dataN)
data.rechunk()
"""

[update.col.remove]
action = "Remove or pop column"
code = "data.drop_in_place('Age')"
details = "Returns the dropped column"


[derive]
section = "Derive"
description = "Generate a modified/alterered/transformed data frame"

[derive.map]
section = "Map"
description = "Transform the dataframe, preservering shape"

[derive.map.col.order]
action = "Reorder all columns in the given order"
code = "data.select(pl.col(['patient', 'sex', 'age']))"

[derive.map.col.order.some]
action = "Reorder specific columns in a given order"
code = """
cols = ['topic', 'parent', 'language']
data.select(pl.col(cols), pl.exclude(cols))
"""
details = "There doesn't seem to be a shorter way to do this"

[derive.map.col.rename]
action = "Rename column"
code = "data.rename({'old': 'new'})"

[derive.map.col.renames]
action = "Rename columns"
code = "data.rename({'old1': 'new1', 'old2': 'new2'})"

[derive.map.col.type]
action = "Cast column type"
code = "data.with_columns(pl.col('col1').cast(pl.Float32))"

[derive.map.col.types]
action = "Cast columns to types"
code = "data.cast({'col1': pl.Float32, 'col2': pl.UInt8})"

[derive.map.col.values]
action = "Update column values"
code = "data.with_columns(pl.col('age') + 5)"

[derive.map.col.values.cond]
action = "Update column values on condition"
code = """
df.with_columns(
    pl.when(pl.col('age') >= 18).
    then(pl.lit(1)).
    otherwise(pl.lit(-1))
)
"""

[derive.map.col.values.conds]
action = "Update column values on conditions"
code = """
df.with_columns(
    pl.when(pl.col('age') >= 18).
    then(pl.lit(1)).
    when(pl.col('Sex') == 'M').
    then(4).
    otherwise(pl.lit(-1))
)
"""

[derive.map.col.values.rows]
action = "Update column values for specific rows"
code = """
rows = [1, 3, 5]
data.with_row_count().with_columns(
    pl.when(pl.col('row_nr').is_in(rows)).
    then(pl.lit(True)).
    otherwise(pl.lit(False))
)
"""

[derive.map.fill.null.zero]
action = "Fill nulls with zero"
code = "data.fill_null(strategy = 'zero')"

[derive.map.fill.null.value]
action = "Fill nulls with value"
code = "data.fill_null(value)"

[derive.map.fill.null.locf]
action = "Fill nulls with LOCF"
code = "data.fill_null(strategy='forward')"
details = "Wrong for grouped data"

[derive.map.fill.'nan'.value]
action = "Fill NaNs with value"
code = "data.fill_nan(value)"

[derive.map.sort.col]
action = "Sort table by column"
code = "data.sort('col1')"


[derive.grow]
section = "Grow"
description = "Transformations which increase the shape of the data frame"

[derive.grow.append.col.constant]
action = "Append constant numeric column"
code = "data.with_columns(Intercept=pl.lit(1))"

[derive.grow.append.col.series]
action = "Append series as new column"
code = """
s = pl.Series("apple", [10, 20, 30])
data.hstack([s])
"""
details = "Note the brackets"

[derive.grow.transform.col]
section = "Add transformed column(s)"

[derive.grow.transform.col.single]
action = "Transform another column"
code = "data.with_columns(AgeSq = pl.col('Age') ** 2)"

[derive.grow.transform.col.single.alias]
code = """
data.with_columns(
    (pl.col('Age') ** 2).alias('AgeSq')
"""

[derive.grow.transform.col.multi]
action = "Multiple transformations from another column"
code = """
data.with_columns(
    Age2=pl.col('Age') ** 2
    Age3=pl.col('Age') ** 3
)
"""

[derive.grow.transform.col.multi.alias]
code = """
data.with_columns(
    (pl.col('Age') ** 2).alias('Age2')
    (pl.col('Age').alias('Age3') ** 3
)
"""
details = "`alias()` can be called after `pl.col()`, may be more readable sometimes"

[derive.grow.transform.mask.pairwise]
action = "Boolean mask based on pairwise comparison of another column"
code = """
data.with_columns(
    CanLive=pl.col('Income') * 3 > pl.col('Rent')
)
"""

[derive.grow.transform.values.cond]
action = "Constant values conditional on another column"
code = """
data.with_columns(
    AdultScore=pl.when(pl.col('age') >= 18).
        then(pl.lit(1)).
        otherwise(pl.lit(-1))
)
"""

[derive.grow.transform.map]
action = "Map another column"
code = """
map = {1: 'a', 2: 'b', 3: 'c'}
data.with_columns(
    NumCat=pl.col('Num').replace_strict(map).cast(pl.Categorical)
)
"""


[derive.grow.transform.int]
section = "Integer column transformations"

[derive.grow.transform.int.datetime.epoch.seconds]
action = "Timestamp (in seconds) since Unix epoch (1970-01-01), as datetime column"
code = """
data.with_columns(
    date=pl.from_epoch('timestamp')
)
"""

[derive.grow.transform.int.date.epoch.days]
action = "Total days since Unix epoch, as date column"
code = """
data.with_columns(
    date=pl.from_epoch('daystamp', time_unit='d')
)
"""

[derive.grow.transform.int.date.epoch.days.cast]
code = """
data.with_columns(
    date=pl.col('daystamp').cast(pl.Date)
)
"""

[derive.grow.transform.int.datetime.epoch.ms]
action = "Milliseconds timestamp since Unix epoch, as datetime column"
code = """
data.with_columns(
    date=pl.from_epoch('timestamp_ms', time_unit='ms')
)
"""

[derive.grow.transform.int.date.offset]
action = "Number of days since reference date, as date column"
code = """
from datetime import date
ref_days = (x - date.fromtimestamp(0)).days
data.with_columns(
    date=pl.col('day').cast(pl.Date) + pl.duration(days=ref_days)
)
"""

[derive.grow.transform.int.date.offset.duration]
code = """
from datetime import date
ref_days = (x - date.fromtimestamp(0)).days
data.with_columns(
    date=(pl.col('day') + ref_days).cast(pl.Date)
)
"""

[derive.grow.transform.str]
section = "String column transformations"

[derive.grow.transform.str.date]
action = "Parse string column to date"
code = """
data.with_columns(
    date=pl.col('datestring').str.to_date()
)
"""

[derive.grow.transform.str.datetime.format]
action = "Parse string column to date with known format"
code = """
data.with_columns(
    date = pl.col('datestring').str.to_date('%Y-%m-%d')
)
"""


[derive.grow.transform.datetime]
section = "Datetime column transformations"

[derive.grow.transform.datetime.weekday]
action = "Weekday from date column"
code = "?"

[derive.grow.transform.datetime.month]
action = "Month from date column"
code = """
data.with_columns(
    month=pl.col('date').dt.month()
)
"""

[derive.grow.transform.datetime.year]
action = "Year from date column"
code = """
data.with_columns(
    year=pl.col('Date').dt.year()
)
"""

[derive.grow.transform.agg.group]
action = "Group-wise from aggregate value"
code = """
data.with_columns(
    elapsedDays=(
        pl.col('Date') - pl.col('Date').min().over('Subject')
    ).dt.total_days().add(1)
)
"""

[derive.grow.row]
section = "Add rows"

[derive.grow.row.tuple]
action = "Add row as tuple"
code = "?"

[derive.grow.row.list.tuple]
action = "Add list of tuples"
code = "?"

[derive.grow.row.dataframe]
action = "Add data frame"
code = "pl.concat(data, df2)"



[derive.shrink]
section = "Shrink the data frame"


[derive.shrink.col]
section = "Remove columns"

[derive.shrink.col.single]
action = "Remove column"
code = "data.drop('Age')"

[derive.shrink.col.multi]
action = "Remove columns"
code = "data.drop(['Age', 'Sex'])"

[derive.shrink.col.multi.select]
code = "data.select(pl.exclude(['Age', 'Sex']))"

[derive.shrink.col.keep.single]
action = "Keep a single column"
code = "data.select(pl.col('Age'))"

[derive.shrink.col.keep.multi]
action = "Keep multiple columns (column subset)"
code = "data.select(pl.col(['Age', 'Sex'])"

[derive.shrink.col.numeric]
action = "Remove all numeric columns"
code = "data.drop(cs.numeric())"


[derive.shrink.row]
section = "Remove rows"

[derive.shrink.row.index]
action = "Remove rows by row numbers"
code = "data.filter(~pl.arange(0, pl.count()).is_in([1, 5, 7]))"

[derive.shrink.row.index.r]
code = "data.with_row_index().filter(~pl.col('index').is_in([1, 5, 7]))"
details = "Output contains additional column 'Index'"

[derive.shrink.row.index.except]
action = "Remove all rows except the given row numbers"
code = "data[[1, 6]]"

[derive.shrink.row.index.except.filter]
code = "data.filter(pl.arange(0, pl.count()).is_in([1, 5, 7]))"


[derive.reshape]
section = "Reshape"

[derive.reshape.long]
action = "From wide to long format"
code = "data.melt(id_vars='sex', value_vars=['a', 'b'])"

[derive.reshape.narrow]
action = "To narrow format"
code = "data.explode(?)"
details = "?"


[merge]
section = "Merge two data frames"

[merge.sorted]
action = "Merge two data frames on the sorted key"
code = "data.merge(data2)"

[merge.inner]
action = "Inner join"
code = "data.join(data2, on = ['sex', 'country'])"

[merge.left]
action = "Left join"
code = "data.join(data2, on = ['sex', 'country'], how = 'left')"

[merge.right]
action = "Right join"
code = "data.join(data2, on = ['sex', 'country'], how = 'right')"

[merge.outer]
action = "Outer (full) join"
code = "data.join(data2, on = ['sex', 'country'], how = 'full')"

[merge.cross]
action = "Cross join"
code = "data.join(data2, on = ['sex', 'country'], how = 'cross')"

[merge.semi]
action = "Semi join (one match per index)"
code = "data.join(data2, on = ['sex', 'country'], how = 'semi')"

[merge.anti]
action = "Anti join (exclude matches from table 2)"
code = "data.join(data2, on = ['sex', 'country'], how = 'anti')"


[extract]
section = "Extract"

[extract.col.series]
action = "Get column as series"
code = "data['col1']"

[extract.col.list]
action = "Get column as list" 
code = "list(data['col1'])"
details = "?"

[extract.row.tuple]
action = "Get _i_ th row as tuple"
code = "data.row(i)"

[extract.rows.list.tuple]
action = "Get rows as list of tuple"
code = "data.rows(...)"
details = "?"

[extract.cell.first]
action = "First item (cell)"
code = "data.item(0, 0)"

[extract.cell.at]
action = "Item (cell) from row _i_ and column index _j_"
code = "data.item(i, j)"

[extract.cell.at.col]
action = "Item (cell) from row _i_ and column name _name_"
code = "data.item(i, name)"


[convert]
section = "Convert"

[convert.lazy]
action = "To `polars.LazyFrame`"
code = "data.lazy()"

[convert.pandas]
action = "To `pandas.DataFrame`"
code = "data.to_pandas()"

[convert.list.series]
action = "To list of series"
code = "data.get_columns()"

[convert.split.dataframes.list.col]
action = "Split into list of data frames based on column"
code = "data.partition_by('sex')"

[convert.split.dataframes.list.cols]
action = "Split into list of data frames based on column tuples"
code = "data.partition_by('sex', 'country')"

[convert.split.dataframes.dict]
action = "Split into dict of data frames based on column(s)"
code = "data.partition_by('sex', 'country', as_dict = True)"

[convert.format.csv]
action = "To CSV file"
code = "data.write_csv('derp.csv')"

[convert.format.parquet]
action = "To Parquet file"
code = "data.write_parquet('derp.parquet')"

[convert.format.json]
action = "To JSON"
code = "?"


[print]
section = "Print"

[print.rows.more]
action = "Print more rows"
code = "with pl.Config(tbl_rows=100): data"
details = "Consider persistent setting: `pl.Config().set_tbl_rows(100)`"
