language = "python"
parent = "python.polars"
name = "dataframe"
title = "Polars DataFrame"
description = """
The polars package provides a fast and powerful implementation of data frames.
"""
details = """
Start a lazy query using a LazyFrame by `data.lazy()`.
Operations on a LazyFrame are not executed until this is requested by either calling `collect()` or `fetch()`.
Lazy operations are advised because they allow for query optimization and more parallelization.
"""
code = "import polars as pl"

[create]
section = "Create"
description = "Create a new DataFrame"

[create.lists]
action = "From column lists"
code = "pl.DataFrame({'A': [1, 2], 'fruits': ['banana', 'apple']})"

[create.list.lists]
action = "From list of lists"
code = """
data = [[1, 2, 3], [4, 5, 6]]
pl.DataFrame(data, schema=['a', 'b', 'c'])
"""

[create.dict]
action = "From dict"
code = "pl.DataFrame(dict)"

[create.dict.schema]
action = "From dict with schema"
code = """
pl.DataFrame(
    dict,
    schema = {'col1': pl.Float32, 'col2': pl.Int64, 'col3': pl.Date}
)
"""

[create.dataframe.arange]
action = "Repeat each row with an index between a row-specific range, defined by min (inclusive) and max (exclusive) columns"
code = """
data.select(
    pl.col('patient'),
    pl.int_ranges(start='startDay', end='endDay').alias('day')
).explode('day')
"""

[create.pandas]
action = "From `pandas.DataFrame`"
code = "pl.from_pandas(data)"

[create.nparray]
action = "From numpy array"
code = """
data = np.array([[1, 2], [3, 4]])\n
pl.DataFrame(data, schema = ['a', 'b'], orient = 'col')
"""

[create.file]
section = "From file formats"

[create.file.csv]
action = "From CSV file"
code = "pl.read_csv('file.csv')"


[test]
section = "Test"

[test.empty]
action = "Empty (no rows)"
code = "data.is_empty()"

[test.equal]
action = "Data frames are equal"
code = "data.equals(data2)"


[test.col]
section = "Columns"

[test.col.contains]
action = "Contains column"
code = "'age' in data.columns"

[test.col.contains.multi]
action = "Contains columns"
code = "{'age', 'sex'}.issubset(data.columns)"

[test.col.contains.dyn]
action = "Contains columns _cols_"
code = "set(cols).issubset(data.columns)"

[test.col.missing]
action = "Column is missing"
code = "'age' not in data.columns"

[test.col.equals]
action = "Columns are equal"
code = "?"

[test.col.equal.series]
action = "Columns are equal series"
code = "data['sex'].equals(data['sex2'].alias('sex'))"
details = "Series names must match!"

[test.col.missing.any]
action = "Column has missing value"
code = "data['sex'].is_null().any()"

[test.col.missing.none]
action = "Column has no missing values"
code = "data['sex'].is_not_null().all()"

[test.col.duplicate.none]
action = "Column has no duplicate values"
code = "data['date'].is_unique().all()"

[test.col.duplicate.any]
action = "Column has duplicate values"
code = "data['date'].is_duplicated().any()"

[test.col.duplicates.any.group]
action = "Column has duplicates within groups"
code = """
data.select(
    pl.col('date').is_duplicated().any().over('patient').any()
).item()
"""
details = "Checks whether any group (patient) has duplicate values (dates)"

[test.col.duplicates.any.group.over]
code = """
data.select(
    pl.col('date').is_duplicated().any().over('patient')
).to_series().any()
"""

[test.col.duplicates.any.group.by]
code = """
data.group_by('patient').agg(
    dupe=pl.col('date').is_duplicated().any()
)['dupe'].any()
"""

[test.col.type]
action = "Column is of type"
code = "data.schema['col1'] == dtype"

[test.col.type.bool]
action = "Column is bool type"
code = "data.schema['alive'] == pl.Bool"

[test.col.type.str]
action = "Column is string type"
code = "data.schema['sex'] == pl.String"

[test.col.type.number]
action = "Columns is numeric"
code = "data.schema['age'].is_numeric()"

[test.col.type.int]
action = "Column is integer type"
code = "data.schema['age'].is_integer()"

[test.col.type.int64]
action = "Column is standard integer (64-bit)"
code = "data.schema['age'] == pl.Int64"

[test.col.type.float]
action = "Columns is float"
code = "data.schema['age'].is_float()"

[test.col.type.list]
action = "Column is list type"
code = "data.schema['keywords'] == pl.List"

[test.col.type.list.str]
action = "Column is of type list of strings"
code = "data.schema['keywords'] == pl.List(pl.String)"

[test.col.row.gt.all]
action = "Column is consistently rowwise greater than another column"
code = "(data['col1'] > data['col2']).all()"

[test.col.row.gt.any]
action = "Column is sometimes rowwise greater than another column"
code = "(data['col1'] > data['col2']).any()"


[extract]
section = "Extract"

[extract.nrow]
action = "Number of rows"
code = "data.height"

[extract.ncol]
action = "Number of columns"
code = "len(data.columns)"

[extract.col.names]
action = "Column names"
code = "data.columns"

[extract.col.types]
action = "Column types"
code = "data.dtypes"

[extract.col.types.map]
action = "Column-types mapping"
code = "data.schema"

[extract.col.which]
action = "Find column index by name"
code = "data.find_idx_by_name('age')"

[extract.col]
action = "Get column _name_ as series"
code = "data['name']"

[extract.col.list]
action = "Get column _name_ as list"
code = "data['name'].to_list()"

[extract.col.first]
action = "Get first column as series"
code = "data.to_series()"

[extract.col.first.select]
code = "data.select( pl.first() )"

[extract.col.index]
action = "Get the _j_th column as series"
code = "data.to_series(j)"

[extract.col.last]
action = "Get last column as series"
code = "data.select(pl.last())"

[extract.col.count.missing]
action = "Number of missing values per column"
code = "data.null_count()"

[extract.col.count.unique.col]
action = "Number of unique values in a column"
code = "data['col1'].n_unique()"

[extract.row.count.unique.col.multi]
action = "Number of unique rows across columns"
code = "?"

[extract.row.tuple]
action = "Get _i_ th row as tuple"
code = "data.row(i)"

[extract.rows.list.tuple]
action = "Get rows as list of tuple"
code = "data.rows(...)"
details = "?"

[extract.cell.first]
action = "First item (cell)"
code = "data.item(0, 0)"

[extract.cell.at]
action = "Item (cell) from row _i_ and column index _j_"
code = "data.item(i, j)"

[extract.cell.at.col]
action = "Item (cell) from row _i_ and column name _name_"
code = "data.item(i, name)"


[update]
section = "Update"
description = "Update the data frame in-place"

[update.col.replace]
action = "Replace column with another series"
code = "data.replace('age', newAgeSeries)"

[update.append.col.series]
action = "Append column from series"
code = "data.hstack(s, in_place = True)"

[update.insert.col.series]
action = "Insert column from series"
code = "data.insert_at_idx(1, s)"

[update.col.remove]
action = "Remove or pop column"
code = "data.drop_in_place('Age')"
details = "Returns the dropped column"


[update.combine]
section = "Combine"

[update.combine.append.df]
action = "Add rows of a data frame (same columns)"
code = "data.extend(data2)"

[update.combine.append.dfs]
action = "Add rows of several data frames (same columns)"
code = """
data.vstack(data2)
data.vstack(dataN)
data.rechunk()
"""


[derive]
section = "Derive"

[derive.transform]
section = "Transform"
description = "Transform the dataframe, preservering shape"

[derive.transform.col.rename]
action = "Rename column _old_ to _new_"
code = "data.rename({'old': 'new'})"

[derive.transform.col.renames]
action = "Rename columns"
code = "data.rename({'old1': 'new1', 'old2': 'new2'})"

[derive.transform.col.type]
action = "Cast column type"
code = "data.with_columns(pl.col('col1').cast(pl.Float32))"

[derive.transform.col.types]
action = "Cast columns to types"
code = "data.cast({'col1': pl.Float32, 'col2': pl.UInt8})"

[derive.transform.col.values]
action = "Update column values"
code = "data.with_columns( pl.col('age') + 5 )"

[derive.transform.col.values.cond]
action = "Update column values on condition"
code = """
df.with_columns(
    pl.when( pl.col('age') >= 18 ).
    then( pl.lit(1) ).
    otherwise( pl.lit(-1) )
)
"""

[derive.transform.col.values.conds]
action = "Update column values on conditions"
code = """
df.with_columns(
    pl.when( pl.col('age') >= 18 ).
    then( pl.lit(1) ).
    when( pl.col('sex') == 'male' ).
    then(4).
    otherwise( pl.lit(-1) )
)
"""

[derive.transform.col.values.rows]
action = "Update column values for specific rows"
code = """
rows = [1, 3, 5]
data.with_row_count().with_columns(
    pl.when( pl.col('row_nr').is_in(rows) ).
    then( pl.lit(True) )
)
"""


[derive.transform.fill]
section = "Fill"

[derive.transform.fill.null.zero]
action = "Fill nulls with zero"
code = "data.fill_null(strategy = 'zero')"

[derive.transform.fill.null.value]
action = "Fill nulls with value"
code = "data.fill_null(value)"

[derive.transform.fill.null.locf]
action = "Fill nulls with LOCF"
code = "data.fill_null(strategy='forward')"
details = "Wrong for grouped data"

[derive.transform.fill.'nan'.value]
action = "Fill NaNs with value"
code = "data.fill_nan(value)"


[derive.transform.mask]
section = "Mask"
description = "All snippets output a `pl.Series` object. Use `.to_list()` to obtain list output."

[derive.transform.mask.row.duplicate]
action = "Mask indicating duplicate rows"
code = "data.is_duplicated()"

[derive.transform.mask.row.unique]
action = "Mask indicating unique rows"
code = "data.is_unique()"

[derive.transform.mask.col]
action = "Mask based on a column value"
code = "data['age'] > 18"

[derive.transform.mask.pairwise]
action = "Mask based on pairwise comparison of another column"
code = """
data.with_columns(
    pl.col('income') * 3 > pl.col('rent')
).to_series()
"""


[derive.transform.aggregate.rows]
section = "Over consecutive rows"
description = "Aggregate over consecutive rows, e.g., for computing a moving average."

[derive.transform.aggregate.rows.rolling.cmean]
action = "Compute centered rolling mean with window size _w_"
code = """
data.with_columns(
    smoothValue=pl.col('value').
        rolling_mean(window_size=w, min_periods=0, center=True)
)
"""

[derive.transform.aggregate.rows.rolling.cmean.group]
action = "Compute centered rolling mean per group with window size _w_"
code = """
data.with_columns(
    smoothValue=pl.col('value').
        rolling_mean(window_size=w, min_periods=0, center=True).
        over('patient')
)
"""

[derive.transform.aggregate.rows.detrend.cmean]
action = "Detrend a column by group, using a centered rolling mean with window size _w_"
code = """
data.with_columns(
    trend=pl.col('value').
        rolling_mean(window_size=w, min_periods=0, center=True).
        over('patient')
).with_columns(
    detrendedValue=(pl.col('value') - pl.col('trend')).over('Patient')
)
"""

[derive.transform.aggregate.rows.detrend.cmean.keep]
action = "Detrend a column by group but preserve the mean level, using a centered rolling mean with window size _w_"
code = """
data.with_columns(
    trend=pl.col('value').rolling_mean(
        window_size=w, min_periods=0, center=True).over('patient')
).with_columns(
    detrendedValue=(
        pl.col('value') - pl.col('trend') + pl.col('trend').mean()
    ).over('Patient')
"""

[derive.transform.aggregate.time]
section = "Over time"
description = "Aggregation is done with respect to a date/time column (e.g., hourly, daily, weekly)"

[derive.transform.aggregate.time.row]
section = "Row-based"
description = "Rolling computation for each row (i.e., observation in time)"

[derive.transform.aggregate.time.row.solo.right]
action = "Rolling computation for a single column, with right-aligned partial window of max _n_ days and at least _i_ rows"
code = """
data.with_columns(
    pl.col('Hospitalized').rolling_mean(
        by='Date',
        closed='both',
        window_size=f'{n - 1}d',
        min_periods=i
    )
)
"""
details = "Currently only available for `rolling_mean()`, not any of the other `rolling_` functions."

[derive.transform.aggregate.time.row.right]
action = "Rolling computation with right-aligned partial window of max _n_ days"
code = """
new_data = data.rolling(
    index_column='Date',
    period=f'{n}d'
).agg(
    pl.mean('Obs)
)
"""

[derive.transform.aggregate.time.row.right.keep]
action = "Rolling computation with right-aligned partial windows of max _n_ days, keep other columns"
code = """
new_data = data.rolling(
    index_column='Date',
    period=f'{n}d'
).agg(
    pl.exclude(['Date', 'Obs']).last(),
    pl.mean('Obs)
)
"""

[derive.transform.aggregate.time.interval]
section = "Interval-based"
description = "Rolling computation for constant (not dynamic!) intervals. This may introduce additional moments in time."

[derive.transform.aggregate.time.interval.right]
action = "Daily rolling computation with right-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='right',
    closed='both',
    start_by='window'
).agg(
    pl.mean('Obs')
)
"""
details = "To be verified"

[derive.transform.aggregate.time.interval.left]
action = "Daily rolling computation with left-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='left',
    closed='both',
    start_by='datapoint'
).agg(
    pl.mean('Obs')
)
"""
details = "To be verified"

[derive.transform.aggregate.time.interval.right.prop]
action = "Dynamic daily rolling statistic proportional to number of window observations, with right-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='right',
    closed='both',
    start_by='window'
).agg(
    pl.sum('Hospitalized').alias('DaysHospitalized'),
    pl.count()
).with_columns(
    Proportion=pl.col('DaysHospitalized') / pl.col('count')
)
"""
details = "To be verified"


[derive.order]
section = "Order"

[derive.order.col]
action = "Reorder all columns in the given order"
code = "data.select(pl.col(['patient', 'sex', 'age']))"

[derive.order.col.some]
action = "Reorder specific columns in a given order"
code = """
cols = ['topic', 'parent', 'language']
data.select(pl.col(cols), pl.exclude(cols))
"""
details = "There doesn't seem to be a shorter way to do this"

[derive.order.row.sort.col]
action = "Sort rows by column value"
code = "data.sort('col1')"


[derive.grow]
section = "Grow"
description = "Transformations which increase the shape of the data frame"

[derive.grow.append.col.constant]
action = "Append constant numeric column"
code = "data.with_columns(Intercept=pl.lit(1))"

[derive.grow.append.col.series]
action = "Append series as new column"
code = """
s = pl.Series("apple", [10, 20, 30])
data.hstack([s])
"""
details = "Note the brackets"


[derive.grow.col]
section = "Add derived column(s)"

[derive.grow.col.single]
action = "Transform another column"
code = "data.with_columns( ageSq = pl.col('age') ** 2 )"

[derive.grow.col.single.alias]
code = """
data.with_columns(
    (pl.col('age') ** 2).alias('ageSq')
)
"""

[derive.grow.col.single.cond]
action = "Constant values conditional on another column"
code = """
data.with_columns(
    AdultScore=pl.when(pl.col('age') >= 18).
        then(pl.lit(1)).
        otherwise(pl.lit(-1))
)
"""

[derive.grow.col.multi]
action = "Add multiple columns based on transformations of other columns"
code = """
data.with_columns(
    ageSq=pl.col('age') ** 2
    hospitalizationsSq=pl.col('hospitalizations') ** 2
)
"""

[derive.grow.col.multi.alias]
code = """
data.with_columns(
    (pl.col('Age') ** 2).alias('Age2')
    pl.col('Age').alias('Age3') ** 3
)
"""
details = "`alias()` can be called after `pl.col()`, may be more readable sometimes"


[derive.grow.int]
section = "Integer column derivations"

[derive.grow.int.map]
action = "Map integer values to strings, as categorical column"
code = """
map = {1: 'first', 2: 'second', 3: 'third'}
data.with_columns(
    rank=pl.col('place').replace_strict(map).cast(pl.Categorical)
)
"""

[derive.grow.int.datetime.epoch.seconds]
action = "Timestamp (in seconds) since Unix epoch (1970-01-01), as datetime column"
code = """
data.with_columns(
    date=pl.from_epoch('timestamp')
)
"""

[derive.grow.int.date.epoch.days]
action = "Total days since Unix epoch, as date column"
code = """
data.with_columns(
    date=pl.from_epoch('daystamp', time_unit='d')
)
"""

[derive.grow.int.date.epoch.days.cast]
code = """
data.with_columns(
    date=pl.col('daystamp').cast(pl.Date)
)
"""

[derive.grow.int.datetime.epoch.ms]
action = "Milliseconds timestamp since Unix epoch, as datetime column"
code = """
data.with_columns(
    date=pl.from_epoch('timestamp_ms', time_unit='ms')
)
"""

[derive.grow.int.date.offset]
action = "Number of days since reference date, as date column"
code = """
from datetime import date
ref_days = (x - date.fromtimestamp(0)).days
data.with_columns(
    date=pl.col('day').cast(pl.Date) + pl.duration(days=ref_days)
)
"""

[derive.grow.int.date.offset.duration]
code = """
from datetime import date
ref_days = (x - date.fromtimestamp(0)).days
data.with_columns(
    date=(pl.col('day') + ref_days).cast(pl.Date)
)
"""


[derive.grow.str]
section = "String column derivations"

[derive.grow.str.date]
action = "Parse string column to date"
code = """
data.with_columns(
    date=pl.col('datestring').str.to_date()
)
"""

[derive.grow.str.datetime.format]
action = "Parse string column to date with known format"
code = """
data.with_columns(
    date = pl.col('datestring').str.to_date('%Y-%m-%d')
)
"""


[derive.grow.datetime]
section = "Datetime column derivations"

[derive.grow.datetime.weekday]
action = "Weekday from date column"
code = "?"

[derive.grow.datetime.month]
action = "Month from date column"
code = """
data.with_columns(
    month=pl.col('date').dt.month()
)
"""

[derive.grow.datetime.year]
action = "Year from date column"
code = """
data.with_columns(
    year=pl.col('Date').dt.year()
)
"""


[derive.grow.datetime.elapsed.group]
action = "Elapsed days between date ranges, by group"
code = """
data.with_columns(
    elapsedDays=(
        pl.col('Date') - pl.col('Date').min().over('Subject')
    ).dt.total_days().add(1)
)
"""


[derive.grow.row]
section = "Add rows"

[derive.grow.row.tuple]
action = "Add row as tuple"
code = "?"

[derive.grow.row.list.tuple]
action = "Add list of tuples"
code = "?"

[derive.grow.row.dataframe]
action = "Add data frame"
code = "pl.concat(data, df2)"


[derive.shrink]
section = "Shrink the data frame"


[derive.shrink.col]
section = "Remove one or more columns"

[derive.shrink.col.remove.single]
action = "Remove single column"
code = "data.drop('Age')"

[derive.shrink.col.remove.multi]
action = "Remove columns"
code = "data.drop(['Age', 'Sex'])"

[derive.shrink.col.remove.multi.2]
code = "data.select(pl.exclude(['Age', 'Sex']))"

[derive.shrink.col.select.single]
action = "Select single column"
code = "data.select('col1')"
details = "Verbose"

[derive.shrink.col.select.single.2]
code = "data.select(pl.col('Age'))"

[derive.shrink.col.select.multi]
action = "Multiple columns"
code = "data.select('col1', 'col2')"

[derive.shrink.col.select.multi.list]
action = "Multiple columns, specified by list"
code = "data.select(['col1', 'col2'])"

[derive.shrink.col.select.multi.list.2]
code = "data.select(pl.col(['Age', 'Sex'])"

[derive.shrink.col.numeric]
action = "Remove all numeric columns"
code = "data.drop(cs.numeric())"


[derive.shrink.row]
section = "Remove one or more rows"

[derive.shrink.row.clear]
action = "Remove all rows (clear)"
code = "data.clear()"

[derive.shrink.row.limit]
action = "Limit query to first _n_ rows"
code = "data.limit(n)"

[derive.shrink.row.limit.tail]
action = "Limit query to last _n_ rows"
code = "data.limit(-n)"

[derive.shrink.row.select]
section = "Keep rows"

[derive.shrink.row.select.index]
action = "Select _i_ th row"
code = "data[i]"

[derive.shrink.row.select.index.end]
action = "Select _i_ th last row"
code = "data[-i]"

[derive.shrink.row.select.head]
action = "First _n_ rows (head)"
code = "data.head(n)"

[derive.shrink.row.select.tail]
action = "Last _n_ rows (tail)"
code = "data.tail(n)"

[derive.shrink.row.select.slice]
action = "Slice of rows from _a_ to _b_"
code = "data[a:b]"

[derive.shrink.row.select.slice.fun]
code = "data.slice(a, b)"

[derive.shrink.row.select.index.list]
action = "Select rows by list of row numbers"
code = "data[rows]"

[derive.shrink.row.select.cond.col]
action = "Keep rows conditionally on column"
code = "data.filter(pl.col('age') >= 18)"

[derive.shrink.row.select.cond.cols]
action = "Keep rows based on multiple column conditions"
code = """
data.filter(
    (pl.col('age') >= 18) & (pl.col('sex') == 'male')
)
"""

[derive.shrink.row.select.sample]
action = "Sample at most _n_ rows"
code = "data.sample(n)"

[derive.shrink.row.select.sample.wr]
action = "Sample _n_ rows, with replacement"
code = "data.sample(n, with_replacement=True)"
details = "Replicates some rows. The result is guaranteed to have _n_ rows, even when the input contains fewer rows."
source = "https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.sample.html"

[derive.shrink.row.select.sample.group]
action = "Sample at most _n_ rows per group"
code = """
data.filter(
    pl.int_range(pl.len()).shuffle().over('patient') < n
)
"""
details = "Groups with fewer than _n_ rows keep all rows."
source = "https://stackoverflow.com/a/72636610/22638740"

[derive.shrink.row.select.sample.wr.group]
action = "Sample _n_ rows per group, with replacement"
code = """
data.group_by('Patient').map_groups(
    lambda df: df.sample(n, with_replacement=True)
)
"""
details = "Very slow due to the `map_groups()` call."


[derive.shrink.row.remove]
section = "Exclude rows"

[derive.shrink.row.remove.index]
action = "Remove rows by row numbers"
code = """
data.with_row_index().filter(
    ~pl.col('index').is_in(rows)
)
"""
details = "Output contains additional column 'Index'"

[derive.shrink.row.remove.index.except]
action = "Remove all rows except the given row numbers"
code = "data[[1, 6]]"

[derive.shrink.row.remove.index.except.filter]
code = "data.filter(pl.arange(0, pl.count()).is_in([1, 5, 7]))"

[derive.shrink.row.remove.null]
action = "Remove rows that contain null values"
code = "data.drop_nulls()"

[derive.shrink.row.remove.null.col]
action = "Remove rows that contain null values in certain columns"
code = "data.drop_nulls(['fruits', 'cars'])"



[derive.shrink.aggregate]
section = "Aggregate rows"

[derive.shrink.aggregate.group]
section = "By group"

[derive.shrink.aggregate.group.mean]
action = "Mean of column by group"
code = "data.group_by('sex').agg(pl.col('age').mean())"


[derive.reshape]
section = "Reshape"

[derive.reshape.long]
action = "From wide to long format"
code = "data.melt(id_vars='sex', value_vars=['a', 'b'])"

[derive.reshape.narrow]
action = "To narrow format"
code = "data.explode(?)"
details = "?"


[derive.combine]
section = "Combine"

[derive.combine.concat]
action = "Concatenate rows of dataframes"
code = "pl.concat([df, df2, dfN])"

[derive.combine.concat.mix]
action = "Concatenate rows of dataframes, having partially overlapping columns"
code = "pl.concat([df, df2, dfN], how = 'diagonal')"

[derive.combine.merge.sorted]
action = "Merge two data frames on the sorted key"
code = "data.merge(data2)"

[derive.combine.merge.inner]
action = "Inner join"
code = "data.join(data2, on = ['sex', 'country'])"

[derive.combine.merge.left]
action = "Left join"
code = "data.join(data2, on = ['sex', 'country'], how = 'left')"

[derive.combine.merge.right]
action = "Right join"
code = "data.join(data2, on = ['sex', 'country'], how = 'right')"

[derive.combine.merge.outer]
action = "Outer (full) join"
code = "data.join(data2, on = ['sex', 'country'], how = 'full')"

[derive.combine.merge.cross]
action = "Cross join"
code = "data.join(data2, on = ['sex', 'country'], how = 'cross')"

[derive.combine.merge.semi]
action = "Semi join (one match per index)"
code = "data.join(data2, on = ['sex', 'country'], how = 'semi')"

[derive.combine.merge.anti]
action = "Anti join (exclude matches from table 2)"
code = "data.join(data2, on = ['sex', 'country'], how = 'anti')"


[convert]
section = "Convert"

[convert.lazy]
action = "To `polars.LazyFrame`"
code = "data.lazy()"

[convert.pandas]
action = "To `pandas.DataFrame`"
code = "data.to_pandas()"

[convert.list.series]
action = "To list of series"
code = "data.get_columns()"

[convert.split.dataframes.list.col]
action = "Split into list of data frames based on column"
code = "data.partition_by('sex')"

[convert.split.dataframes.list.cols]
action = "Split into list of data frames based on column tuples"
code = "data.partition_by('sex', 'country')"

[convert.split.dataframes.dict]
action = "Split into dict of data frames based on column(s)"
code = "data.partition_by('sex', 'country', as_dict = True)"

[convert.format.csv]
action = "To CSV file"
code = "data.write_csv('derp.csv')"

[convert.format.parquet]
action = "To Parquet file"
code = "data.write_parquet('derp.parquet')"

[convert.format.json]
action = "To JSON"
code = "?"


[show]
section = "Show"

[shows.rows.more]
action = "Print more rows"
code = "with pl.Config(tbl_rows=100): data"
details = "Consider persistent setting: `pl.Config().set_tbl_rows(100)`"
